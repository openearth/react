{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netCDF4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnetCDF4\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnc\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxarray\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mxr\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'netCDF4'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm, colors\n",
    "import hydromt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import descartes # required to plot polygons\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% set paths\n",
    "path = r\"c:\\Users\\harezlak\\OneDrive - Stichting Deltares\\Deltares\\REACT\\Software\\Tutorial\\Tutorial files\\wflow_model_updated\"\n",
    "root = r\"c:\\Users\\harezlak\\OneDrive - Stichting Deltares\\Deltares\\REACT\\Software\\Tutorial\\Tutorial files\\wflow_model_updated\"\n",
    "nc_name = 'output.nc'\n",
    "nc_file  = os.path.join(path, 'run_default', nc_name) # netcdf file met wflow output\n",
    "mod = hydromt.WflowModel(root, mode='r')\n",
    "output =r\"c:\\Users\\harezlak\\OneDrive - Stichting Deltares\\Deltares\\REACT\\Software\\Tutorial\\Tutorial files\\wflow_model_updated\\output\"\n",
    "AllDataPath = os.path.join(output, 'Output_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% user defined parameters\n",
    "timestepsecs = '86400'  # timestep in seconds. see *toml file in wflow model folder\n",
    "refD = '19000101' # reference date. see *toml file in wflow model folder\n",
    "start_year = 2009 # manually enter start year of statistics calculation, this should be a year for which discharges are available for the whole year\n",
    "num_years = 1 # manually enter the amount of years which are completely available\n",
    "write_output = True # schrijf output van flow indicators weg per jaar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    dimensions(sizes): time(731), lon(96), lat(183), layer(4)\n",
      "    variables(dimensions): float64 lon(lon), float64 lat(lat), float64 layer(layer), float64 time(time), float32 int(time, lat, lon), float32 river_width(time, lat, lon), float32 q_land(time, lat, lon), float32 q_river(time, lat, lon), float32 water_depth(time, lat, lon), float32 h_land(time, lat, lon), float32 precip(time, lat, lon)\n",
      "    groups: \n",
      "{}\n",
      "<class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'time', size = 731\n",
      "<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 96\n",
      "<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 183\n",
      "<class 'netCDF4._netCDF4.Dimension'>: name = 'layer', size = 4\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float64 lon(lon)\n",
      "    _FillValue: nan\n",
      "    long_name: longitude\n",
      "    standard_name: longitude\n",
      "    axis: X\n",
      "    units: degrees_east\n",
      "unlimited dimensions: \n",
      "current shape = (96,)\n",
      "filling on\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float64 lat(lat)\n",
      "    _FillValue: nan\n",
      "    long_name: latitude\n",
      "    standard_name: latitude\n",
      "    axis: Y\n",
      "    units: degrees_north\n",
      "unlimited dimensions: \n",
      "current shape = (183,)\n",
      "filling on\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float64 layer(layer)\n",
      "    long_name: layer_index\n",
      "    standard_name: layer_index\n",
      "    axis: Z\n",
      "unlimited dimensions: \n",
      "current shape = (4,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float64 time(time)\n",
      "    units: days since 1900-01-01 00:00:00\n",
      "    calendar: proleptic_gregorian\n",
      "unlimited dimensions: time\n",
      "current shape = (731,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 int(time, lat, lon)\n",
      "    _FillValue: nan\n",
      "unlimited dimensions: time\n",
      "current shape = (731, 183, 96)\n",
      "filling on\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 river_width(time, lat, lon)\n",
      "    _FillValue: nan\n",
      "unlimited dimensions: time\n",
      "current shape = (731, 183, 96)\n",
      "filling on\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 q_land(time, lat, lon)\n",
      "    _FillValue: nan\n",
      "unlimited dimensions: time\n",
      "current shape = (731, 183, 96)\n",
      "filling on\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 q_river(time, lat, lon)\n",
      "    _FillValue: nan\n",
      "unlimited dimensions: time\n",
      "current shape = (731, 183, 96)\n",
      "filling on\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 water_depth(time, lat, lon)\n",
      "    _FillValue: nan\n",
      "unlimited dimensions: time\n",
      "current shape = (731, 183, 96)\n",
      "filling on\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 h_land(time, lat, lon)\n",
      "    _FillValue: nan\n",
      "unlimited dimensions: time\n",
      "current shape = (731, 183, 96)\n",
      "filling on\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 precip(time, lat, lon)\n",
      "    _FillValue: nan\n",
      "unlimited dimensions: time\n",
      "current shape = (731, 183, 96)\n",
      "filling on\n"
     ]
    }
   ],
   "source": [
    "#%% inspect data and variables\n",
    "ds = nc.Dataset(nc_file)\n",
    "print(ds)\n",
    "print(ds.__dict__)\n",
    "for dim in ds.dimensions.values():\n",
    "    print(dim)\n",
    "for var in ds.variables.values():\n",
    "    print(var)   \n",
    " \n",
    "    \n",
    "reference_date = datetime.datetime(int((refD[0:4])), int((refD[4:6])), int((refD[6:8])))        \n",
    "# determine start end end dates\n",
    "time = ds['time'][:]  \n",
    "startdate = reference_date +datetime.timedelta(days=time[0])\n",
    "enddate = reference_date +datetime.timedelta(days=time[len(time)-1]) \n",
    "timestep = timestepsecs + 'S' \n",
    "date_range = pd.date_range(start = startdate, end = enddate, freq = timestep)   \n",
    "\n",
    "startDmdu = time[0]  # start date of simulation with reference to refD\n",
    "endDmdu = time[len(time)-1]  # end date of simulation with reference to refD\n",
    "TotalSimTime =   endDmdu - startDmdu \n",
    "\n",
    "Dates_df = pd.DataFrame(data=date_range)\n",
    "Dates_df = Dates_df.rename(columns={0: 'date'}) \n",
    "Dates_df['year'], Dates_df['month'], Dates_df['day'] = Dates_df['date'].dt.year, Dates_df['date'].dt.month, Dates_df['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% extract required data and put in table next to lat long coordinates\n",
    "\n",
    "# extract coordinates\n",
    "lat = ds['lat'][:] \n",
    "long = ds['lon'][:] \n",
    "lat = pd.DataFrame(data=lat).iloc[::-1]    \n",
    "\n",
    "\n",
    "cors = []\n",
    "# create list of lat long coordinates in vector format\n",
    "for i in range(len(long)):\n",
    "    df1 = pd.DataFrame(lat)\n",
    "    df2 = long[i]\n",
    "    df2 = pd.DataFrame(np.repeat(df2, len(lat), axis=0))\n",
    "    frames = [df1, df2]\n",
    "    data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    cors.append(data)\n",
    "final_cors = pd.concat(cors, axis = 0, ignore_index = True)  \n",
    "\n",
    "# add digital elevation, river slope, river width, river height and stream order to coordinates\n",
    "\n",
    "# extract DEM data for all coordinates\n",
    "da =mod.staticmaps['wflow_dem'].values\n",
    "data_step = pd.DataFrame(data=da)\n",
    "dem_data = []\n",
    "for i in range(len(long)):\n",
    "    df3 = pd.DataFrame(data_step[i])\n",
    "    frames = [df3]\n",
    "    data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    dem_data.append(data)\n",
    "dem_complete = pd.concat(dem_data, axis = 0, ignore_index = True)      \n",
    "    \n",
    "# extract river slope data for all coordinates   \n",
    "da =mod.staticmaps['RiverSlope'].values\n",
    "data_step = pd.DataFrame(data=da)\n",
    "slope_data = []\n",
    "for i in range(len(long)):\n",
    "    df3 = pd.DataFrame(data_step[i])\n",
    "    frames = [df3]\n",
    "    data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    slope_data.append(data)\n",
    "slope_complete = pd.concat(slope_data, axis = 0, ignore_index = True)     \n",
    "    \n",
    "\n",
    "# extract river width data for all coordinates\n",
    "da =mod.staticmaps['wflow_riverwidth'].values\n",
    "data_step = pd.DataFrame(data=da)\n",
    "river_width_data = []\n",
    "for i in range(len(long)):\n",
    "    df3 = pd.DataFrame(data_step[i])\n",
    "    frames = [df3]\n",
    "    data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    river_width_data.append(data)\n",
    "river_width_complete = pd.concat(river_width_data, axis = 0, ignore_index = True)   \n",
    "\n",
    "# extract river width data for all coordinates\n",
    "da =mod.staticmaps['wflow_streamorder'].values\n",
    "data_step = pd.DataFrame(data=da)\n",
    "stream_order_data = []\n",
    "for i in range(len(long)):\n",
    "    df3 = pd.DataFrame(data_step[i])\n",
    "    frames = [df3]\n",
    "    data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    stream_order_data.append(data)\n",
    "stream_order_complete = pd.concat(stream_order_data, axis = 0, ignore_index = True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create lookup table for flood timing\n",
    "months = pd.DataFrame([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,12])\n",
    "startday = pd.DataFrame([1, 32, 61, 92, 122, 153, 183, 214, 245, 275, 306, 336])\n",
    "frames = [months, startday]\n",
    "time_table = pd.concat(frames, axis = 1, ignore_index = True)\n",
    "time_table.rename(columns={0: 'fldtime', 1: 'startday'}, inplace=True)   \n",
    "\n",
    "\n",
    "qr = ds['q_river'][:,:,:]   # extract river discharge\n",
    "\n",
    "Q_indicators = []\n",
    "for y in range(num_years): # loop over years\n",
    "    cur_year = start_year+y\n",
    "    temp_year_store_BFI = [] # temporal dataframe to store annual data\n",
    "    temp_month_store = [] # store montly ratios in lat long format\n",
    "    \n",
    "    # extract discharge per year    \n",
    "    year_index = Dates_df.index[(Dates_df['year'] == cur_year)]\n",
    "    dates = Dates_df[(Dates_df['year'] == cur_year)]# extract dates per year    \n",
    "    dates.reset_index(inplace = True, drop = True)# reset index for later concatenation \n",
    "    Q_year = qr[year_index] # extract discharges of this year\n",
    "    \n",
    "    n_zeros = np.count_nonzero(Q_year==0, axis=0) # count number of zero flow days for each grid cell over the year\n",
    "    # turn dataframe upside down to match the static data..?\n",
    "    n_zeros = pd.DataFrame(data=n_zeros).iloc[::-1]    \n",
    "    \n",
    "    size = np.shape(Q_year)\n",
    "    Q_daily_mean = np.mean(Q_year, axis = 0) # annual mean daily discharge per cell\n",
    "    # turn dataframe upside down to match the static data..?\n",
    "    Q_daily_mean = pd.DataFrame(data=Q_daily_mean).iloc[::-1]    \n",
    "    \n",
    "    Q_daily_stdev = np.std(Q_year, axis = 0)# standard deviation of mean annual daily discharges\n",
    "    # turn dataframe upside down to match the static data..?\n",
    "    Q_daily_stdev = pd.DataFrame(data=Q_daily_stdev).iloc[::-1]      \n",
    "    \n",
    "    DAYCV = Q_daily_stdev/Q_daily_mean# coefficient of variation calculated as ratio between stdev and mean \n",
    "    \n",
    "    \n",
    "    Q_threshold = [] # threshold value for flood comparison\n",
    "    number_no_flow_days = []\n",
    "    coef_var = []\n",
    "    # put flood threshold in right lat-long format\n",
    "    for i in range(len(long)):\n",
    "        df3 = pd.DataFrame(Q_daily_mean[i])\n",
    "        df2 = pd.DataFrame(n_zeros[i])\n",
    "        df4 = pd.DataFrame(DAYCV[i])\n",
    "        frames = [df3]\n",
    "        frames2 = [df2]\n",
    "        frames3 = [df4]\n",
    "        data = pd.concat(frames, axis = 1, ignore_index = True)   \n",
    "        data2 = pd.concat(frames2, axis = 1, ignore_index = True)  \n",
    "        data3 = pd.concat(frames3, axis = 1, ignore_index = True)  \n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        data2.reset_index(drop=True, inplace=True)\n",
    "        data3.reset_index(drop=True, inplace=True)\n",
    "        Q_threshold.append(data)\n",
    "        number_no_flow_days.append(data2)\n",
    "        coef_var.append(data3)\n",
    "    Q_threshold_data = pd.concat(Q_threshold, axis = 0, ignore_index = True)     \n",
    "    no_flow_days = pd.concat(number_no_flow_days, axis = 0, ignore_index = True)  \n",
    "    coefficient_variation = pd.concat(coef_var, axis = 0, ignore_index = True)  \n",
    "    \n",
    "\n",
    "    data_compare_threshold = []\n",
    "    daily_discharge_data = []\n",
    "    for f in range(size[0]): # loop over daily discharges to check for flooding frequency\n",
    "        Current_day_data = Q_year[f,:,:] # extract daily data\n",
    "        # turn dataframe upside down to match the static data..?\n",
    "        Current_day_data = pd.DataFrame(data=Current_day_data).iloc[::-1]\n",
    "        # loop over lat_long to create right data format\n",
    "        \n",
    "        daily_data = [] # store daily data\n",
    "        for i in range(len(long)):\n",
    "            df3 = pd.DataFrame(Current_day_data[i])\n",
    "            frames = [df3]\n",
    "            data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            daily_data.append(data)\n",
    "        daily_data_complete = pd.concat(daily_data, axis = 0, ignore_index = True)   \n",
    "        \n",
    "        Flood_pres = daily_data_complete>Q_threshold_data # compare daily data with annual mean daily discharge to determine flood presence\n",
    "        Flood_val = Flood_pres*1 # covert bools into numbers\n",
    "        data_compare_threshold.append(Flood_val)\n",
    "        daily_discharge_data.append(daily_data_complete)\n",
    "        \n",
    "    Threshold_data = pd.concat(data_compare_threshold, axis = 1, ignore_index = True)    # convert to dataframe\n",
    "    Daily_discharge_data = pd.concat(daily_discharge_data, axis = 1, ignore_index = True)    # convert to dataframe\n",
    "    dims = np.shape(Threshold_data)\n",
    "\n",
    "        \n",
    "    flood_count = []    \n",
    "    fld_pred_ind = []\n",
    "    fld_time_ind = []\n",
    "    for th in range(dims[0]): # loop over cells to calculate number of floods\n",
    "        Current_data_cell = Threshold_data.iloc[th,:] # extract threshold data for cell\n",
    "        Current_data_cell = pd.DataFrame(Current_data_cell)\n",
    "        #check_zero = int(np.sum(Current_data_cell))\n",
    "        Current_discharge_cell = Daily_discharge_data.iloc[th,:] # extract discharge data for cell\n",
    "        Current_discharge_cell = pd.DataFrame(Current_discharge_cell)\n",
    "        if int(np.sum(Current_data_cell)) == 0: # no floods, put zero in dataframe\n",
    "             flood_count.append(0)\n",
    "             fld_pred_ind.append(0)\n",
    "             fld_time_ind.append(0)\n",
    "             \n",
    "        else:                \n",
    "            # create dataset with floods and corresponding dates\n",
    "            Data_c = pd.concat([dates, Current_data_cell,Current_discharge_cell],axis = 1, ignore_index=True)                \n",
    "            Data_c.rename(columns={0: 'date', 1: 'year', 2:'month', 3:'day', 4: 'flood', 5:'Q'}, inplace=True)                \n",
    "            Data_c['value_grp'] = (Data_c.flood.diff(1) != 0).astype('int').cumsum() # analyse data per gridcell to extract floods and calculate cumsum\n",
    "    \n",
    "            cumsum_floods = pd.DataFrame({'BeginDate' : Data_c.groupby('value_grp').date.first(), 'FloodVal' : Data_c.groupby('value_grp').flood.max(), \n",
    "            'EndDate' : Data_c.groupby('value_grp').date.last(),'BeginMonth' : Data_c.groupby('value_grp').month.first(),\n",
    "            'EndMonth' : Data_c.groupby('value_grp').month.last(),'Q_max' : Data_c.groupby('value_grp').Q.max(),\n",
    "            'Consecutive' : Data_c.groupby('value_grp').size()}).reset_index(drop=True) \n",
    "            \n",
    "            # get the location of the maximum value to extract the day at which it occurs\n",
    "            idx = Data_c.groupby(['value_grp'])['Q'].transform(max) == Data_c['Q']\n",
    "            A_test=Data_c[idx]\n",
    "            cumsum_floods['day_Q_max'] = A_test['day'].reset_index(drop=True)  # add days of max flood\n",
    "            cumsum_floods['month'] = A_test['month'].reset_index(drop=True)# add date of max flood\n",
    "            cumsum_floods['date_Q_max'] = A_test['date'].reset_index(drop=True)# add date of max flood\n",
    "            \n",
    "            # select only floods = Floodval = 1\n",
    "            Only_flood_data = cumsum_floods[(cumsum_floods['FloodVal'] == 1)]\n",
    "            \n",
    "            # calculate the total number of floods exceeding the threshold of this year for this grid cell (Indicator = FLDFREQ)\n",
    "            dim = np.shape(Only_flood_data)\n",
    "            flood_count.append(int(dim[0]/2))\n",
    "    \n",
    "            # calculate proportion of floods within each two month window and calculate the max over all windows\n",
    "            floods_in_months = pd.DataFrame({'Count_floods' : Only_flood_data.groupby('month').Consecutive.count()}) \n",
    "            total_floods = np.sum(floods_in_months)\n",
    "            fldpred_month = []\n",
    "            for f in range(1,11): # loop over months to calculate FLDPRED\n",
    "                fl = floods_in_months.loc[f:f+1]\n",
    "                fldpred = np.sum(fl)/total_floods # calculate flood prediction for two months time window\n",
    "                fldpred_month.append(fldpred)    \n",
    "            fldpred = np.max(fldpred_month) #FLDPRED value\n",
    "            fldpred_dataframe = pd.concat(fldpred_month, axis = 0, ignore_index = True)    # convert to dataframe            \n",
    "            max_fld_predwindow = fldpred_dataframe.idxmax()+1 # FLDTIME value: this is the number corresponding to the interval of months, e.g. 2 =  feb-mar, 5 = may-june etc.\n",
    "            \n",
    "            # convert flood time to actual start date of flood\n",
    "            stdate = time_table[(time_table['fldtime'] == max_fld_predwindow)]\n",
    "            \n",
    "            fld_pred_ind.append(fldpred)\n",
    "            fld_time_ind.append(int(stdate.startday))           \n",
    "\n",
    "    for j in (range(12)): # loop over months      \n",
    "        month_index = Dates_df.index[(Dates_df['year'] == cur_year) & (Dates_df['month'] == j+1)]\n",
    "        Q_month = qr[month_index] # extract discharges of this month\n",
    "        min_month = np.min(Q_month,axis =0)\n",
    "        mean_month = np.mean(Q_month, axis =0)\n",
    "        ratio_month = min_month/mean_month\n",
    "        # turn dataframe upside down to match the static data..?\n",
    "        ratio_month = pd.DataFrame(data=ratio_month).iloc[::-1]\n",
    "        \n",
    "        # rearrange data into lat long format\n",
    "        data_latlong = []\n",
    "        for i in range(len(long)):\n",
    "            df_cur_rat = pd.DataFrame(ratio_month[i])\n",
    "            frames = [df_cur_rat]\n",
    "            data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "            data.reset_index(drop=True, inplace=True)              \n",
    "            data_latlong.append(data)        \n",
    "        data_month = pd.concat(data_latlong, axis = 0, ignore_index = True)           \n",
    "        data_month.reset_index(drop=True, inplace=True)                       \n",
    "        temp_month_store.append(data_month)\n",
    "    temp_year_store_BFI = pd.concat(temp_month_store, axis = 1, ignore_index = True)  \n",
    "    BFI_year = np.mean(temp_year_store_BFI, axis = 1) # base flow index per grid cell of current year\n",
    "    \n",
    "    # todo: save data per year\n",
    "    frames = [pd.DataFrame(BFI_year)*100,pd.DataFrame(flood_count), no_flow_days, pd.DataFrame(fld_pred_ind), pd.DataFrame(fld_time_ind), pd.DataFrame(coefficient_variation)*100]\n",
    "    Q_indicators_year = pd.concat(frames, axis = 1, ignore_index = True)  \n",
    "    Q_indicators_year.rename(columns={0: 'BFI', 1: 'FLDFREQ', 2:'ZERODAY', 3:'FLDPRED', 4: 'FLDTIME', 5: 'DAYCV'}, inplace=True)     \n",
    "    \n",
    "    if write_output == True: # if output is true, then write annual data for flow indicators in file\n",
    "        nameCSV = 'Flow_indicators' + '_year_' + str(y) + '.csv'\n",
    "        AllDataPath = os.path.join(output, nameCSV)  \n",
    "        Q_indicators_year.to_csv(AllDataPath, sep=';', mode=\"w\", header=True, index=False)   \n",
    "        \n",
    "    Q_indicators.append(Q_indicators_year)\n",
    "    \n",
    "\n",
    "# combine all years in a 3d dataframe\n",
    "Total_Q_indicators = np.dstack([Q_indicators])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% # zone selection and calculation of statistics for each zone \n",
    "fr = [final_cors, dem_complete, slope_complete, river_width_complete, stream_order_complete]   \n",
    "All_static_data = pd.concat(fr, axis = 1, ignore_index = True)   \n",
    "All_static_data.rename(columns={0: 'Y', 1: 'X', 2:'elevation', 3:'slope', 4:'river_width', 5:'stream_order'}, inplace=True)\n",
    "All_static_data = All_static_data.replace(-9999.0, np.NaN) # replance missing values by NaNs\n",
    "All_static_data = All_static_data.dropna()# remove rows with Nans\n",
    "MaximumStreamOrder = np.max(All_static_data.stream_order) # find maximum stream order\n",
    "\n",
    "# round coordinates to match right location\n",
    "All_static_data['Y'] = round(All_static_data['Y'],4)\n",
    "All_static_data['X'] = round(All_static_data['X'],4)\n",
    "\n",
    "Zones = []\n",
    "\n",
    "# selection on altitude\n",
    "# ------------- LOW ALTITUDE -------------------\n",
    "LowAltitude = All_static_data.loc[(All_static_data['elevation'] <= 200)]\n",
    "\n",
    "# ----------- Gentle gradient -------------------\n",
    "LA_GentleGradient = LowAltitude.loc[(LowAltitude['slope'] <= 0.02)]\n",
    "\n",
    "LA_GG_Max = LA_GentleGradient.loc[(LA_GentleGradient['stream_order'] == MaximumStreamOrder)]\n",
    "LA_GG_Max['ZoneId'] = 'LA_GG_Max'\n",
    "Zones.append(LA_GG_Max) # add index to list\n",
    "\n",
    "LA_GG_Max1 = LA_GentleGradient.loc[(LA_GentleGradient['stream_order'] == MaximumStreamOrder-1)]\n",
    "LA_GG_Max1['ZoneId'] = 'LA_GG_Max1'\n",
    "Zones.append(LA_GG_Max1) # add index to list\n",
    "\n",
    "LA_GG_Max2 = LA_GentleGradient.loc[(LA_GentleGradient['stream_order'] == MaximumStreamOrder-2)]\n",
    "LA_GG_Max2['ZoneId'] = 'LA_GG_Max2'\n",
    "Zones.append(LA_GG_Max2) # add index to list\n",
    "\n",
    "#------------- riffles -----------------------\n",
    "LA_riffle = LowAltitude.loc[(LowAltitude['slope'] > 0.02) & (LowAltitude['slope'] <= 0.04)]\n",
    "\n",
    "LA_RI_Max = LA_riffle.loc[(LA_riffle['stream_order'] == MaximumStreamOrder)]\n",
    "LA_RI_Max['ZoneId'] = 'LA_RI_Max'\n",
    "Zones.append(LA_RI_Max) # add index to list\n",
    "\n",
    "LA_RI_Max1 = LA_riffle.loc[(LA_riffle['stream_order'] == MaximumStreamOrder-1)]\n",
    "LA_RI_Max1['ZoneId'] = 'LA_RI_Max1'\n",
    "Zones.append(LA_RI_Max1) # add index to list\n",
    "\n",
    "LA_RI_Max2  = LA_riffle.loc[(LA_riffle['stream_order'] == MaximumStreamOrder-2)]\n",
    "LA_RI_Max2['ZoneId'] = 'LA_RI_Max2'\n",
    "Zones.append(LA_RI_Max2) # add index to list\n",
    "\n",
    "# ----------- steep -----------------------\n",
    "LA_steep = LowAltitude.loc[(LowAltitude['slope'] > 0.04) & (LowAltitude['slope'] <= 0.1)]\n",
    "\n",
    "LA_ST_Max = LA_steep.loc[(LA_steep['stream_order'] == MaximumStreamOrder)]\n",
    "LA_ST_Max['ZoneId'] = 'LA_ST_Max'\n",
    "Zones.append(LA_ST_Max) # add index to list\n",
    "\n",
    "LA_ST_Max1 = LA_steep.loc[(LA_steep['stream_order'] == MaximumStreamOrder-1)]\n",
    "LA_ST_Max1['ZoneId'] = 'LA_ST_Max1'\n",
    "Zones.append(LA_ST_Max1) # add index to list\n",
    "\n",
    "LA_ST_Max2 = LA_steep.loc[(LA_steep['stream_order'] == MaximumStreamOrder-2)]    \n",
    "LA_ST_Max2['ZoneId'] = 'LA_ST_Max2'\n",
    "Zones.append(LA_ST_Max2) # add index to list\n",
    "\n",
    "\n",
    "# --------------- very steep ----------------\n",
    "LA_verysteep = LowAltitude.loc[(LowAltitude['slope'] > 0.1)]\n",
    "\n",
    "LA_VS_Max = LA_verysteep.loc[(LA_verysteep['stream_order'] == MaximumStreamOrder)]\n",
    "LA_VS_Max['ZoneId'] = 'LA_VS_Max'\n",
    "Zones.append(LA_VS_Max) # add index to list\n",
    "\n",
    "LA_VS_Max1 = LA_verysteep.loc[(LA_verysteep['stream_order'] == MaximumStreamOrder-1)]\n",
    "LA_VS_Max1['ZoneId'] = 'LA_VS_Max1'\n",
    "Zones.append(LA_VS_Max1) # add index to list\n",
    "\n",
    "LA_VS_Max2 = LA_verysteep.loc[(LA_verysteep['stream_order'] == MaximumStreamOrder-2)]\n",
    "LA_VS_Max2['ZoneId'] = 'LA_VS_Max2'\n",
    "Zones.append(LA_VS_Max2) # add index to list\n",
    "\n",
    "# ----------- MID ALTITUDE -------------------\n",
    "MidAltitude = All_static_data.loc[(All_static_data['elevation'] > 200) & (All_static_data['slope'] <= 800)]\n",
    "\n",
    "# ----------- gentle gradient ------------------\n",
    "MA_GentleGradient = MidAltitude.loc[(MidAltitude['slope'] <= 0.02)]\n",
    "MA_GG_Max = MA_GentleGradient.loc[(MA_GentleGradient['stream_order'] == MaximumStreamOrder)]\n",
    "MA_GG_Max['ZoneId'] = 'MA_GG_Max'\n",
    "Zones.append(MA_GG_Max) # add index to list\n",
    "\n",
    "MA_GG_Max1 = MA_GentleGradient.loc[(MA_GentleGradient['stream_order'] == MaximumStreamOrder-1)]\n",
    "MA_GG_Max1['ZoneId'] = 'MA_GG_Max1'\n",
    "Zones.append(MA_GG_Max1) # add index to list\n",
    "\n",
    "MA_GG_Max2 = MA_GentleGradient.loc[(MA_GentleGradient['stream_order'] == MaximumStreamOrder-2)]\n",
    "MA_GG_Max2['ZoneId'] = 'MA_GG_Max2'\n",
    "Zones.append(MA_GG_Max2) # add index to list\n",
    "\n",
    "#------------- riffles -----------------------\n",
    "MA_riffle= MidAltitude.loc[(MidAltitude['slope'] > 0.02) & (MidAltitude['slope'] <= 0.04)]\n",
    "\n",
    "MA_RI_Max = MA_riffle.loc[(MA_riffle['stream_order'] == MaximumStreamOrder)]\n",
    "MA_RI_Max['ZoneId'] = 'MA_RI_Max'\n",
    "Zones.append(MA_RI_Max) # add index to list\n",
    "MA_RI_Max1 = MA_riffle.loc[(MA_riffle['stream_order'] == MaximumStreamOrder-1)]\n",
    "MA_RI_Max1['ZoneId'] = 'MA_RI_Max1'\n",
    "Zones.append(MA_RI_Max1) # add index to list\n",
    "MA_RI_Max2 = MA_riffle.loc[(MA_riffle['stream_order'] == MaximumStreamOrder-2)]\n",
    "MA_RI_Max2['ZoneId'] = 'MA_RI_Max2'\n",
    "Zones.append(MA_RI_Max2) # add index to list\n",
    "\n",
    "# ----------- steep -----------------------\n",
    "MA_steep = MidAltitude.loc[(MidAltitude['slope'] > 0.04) & (MidAltitude['slope'] <= 0.1)]\n",
    "\n",
    "MA_ST_Max = MA_steep.loc[(MA_steep['stream_order'] == MaximumStreamOrder)]\n",
    "MA_ST_Max['ZoneId'] = 'MA_ST_Max'\n",
    "Zones.append(MA_ST_Max) # add index to list\n",
    "MA_ST_Max1 = MA_steep.loc[(MA_steep['stream_order'] == MaximumStreamOrder-1)]\n",
    "MA_ST_Max1['ZoneId'] = 'MA_ST_Max1'\n",
    "Zones.append(MA_ST_Max1) # add index to list\n",
    "MA_ST_Max2 = MA_steep.loc[(MA_steep['stream_order'] == MaximumStreamOrder-2)]\n",
    "MA_ST_Max2['ZoneId'] = 'MA_ST_Max2'\n",
    "Zones.append(MA_ST_Max2) # add index to list\n",
    "\n",
    "# --------------- very steep ----------------\n",
    "MA_verysteep = MidAltitude.loc[(MidAltitude['slope'] > 0.1)]\n",
    "\n",
    "MA_VS_Max = LA_verysteep.loc[(LA_verysteep['stream_order'] == MaximumStreamOrder)]\n",
    "MA_VS_Max['ZoneId'] = 'MA_VS_Max'\n",
    "Zones.append(MA_VS_Max) # add index to list\n",
    "MA_VS_Max1 = LA_verysteep.loc[(LA_verysteep['stream_order'] == MaximumStreamOrder-1)]\n",
    "MA_VS_Max1['ZoneId'] = 'MA_VS_Max1'\n",
    "Zones.append(MA_VS_Max1) # add index to list\n",
    "MA_VS_Max2 = LA_verysteep.loc[(LA_verysteep['stream_order'] == MaximumStreamOrder-2)]\n",
    "MA_VS_Max2['ZoneId'] = 'MA_VS_Max2'\n",
    "Zones.append(MA_VS_Max2) # add index to list\n",
    "\n",
    "# ----------- High ALTITUDE -------------------\n",
    "HighAltitude = All_static_data.loc[(All_static_data['elevation'] > 800)]\n",
    "\n",
    "# ----------- Gentle gradient -------------------\n",
    "HA_GentleGradient = HighAltitude.loc[(HighAltitude['slope'] <= 0.02)]\n",
    "\n",
    "HA_GG_Max = HA_GentleGradient.loc[(HA_GentleGradient['stream_order'] == MaximumStreamOrder)]\n",
    "HA_GG_Max['ZoneId'] = 'HA_GG_Max'\n",
    "Zones.append(HA_GG_Max) # add index to list\n",
    "HA_GG_Max1 = HA_GentleGradient.loc[(HA_GentleGradient['stream_order'] == MaximumStreamOrder-1)]\n",
    "HA_GG_Max1['ZoneId'] = 'HA_GG_Max1'\n",
    "Zones.append(HA_GG_Max1) # add index to list\n",
    "HA_GG_Max2 = HA_GentleGradient.loc[(HA_GentleGradient['stream_order'] == MaximumStreamOrder-2)]\n",
    "HA_GG_Max2['ZoneId'] = 'HA_GG_Max2'\n",
    "Zones.append(HA_GG_Max2) # add index to list\n",
    "\n",
    "#------------- riffles -----------------------\n",
    "HA_riffle = HighAltitude.loc[(HighAltitude['slope'] > 0.02) & (HighAltitude['slope'] <= 0.04)]\n",
    "\n",
    "HA_RI_Max = HA_riffle.loc[(HA_riffle['stream_order'] == MaximumStreamOrder)]\n",
    "HA_RI_Max['ZoneId'] = 'HA_RI_Max'\n",
    "Zones.append(HA_RI_Max) # add index to list\n",
    "HA_RI_Max1 = HA_riffle.loc[(HA_riffle['stream_order'] == MaximumStreamOrder-1)]\n",
    "HA_RI_Max1['ZoneId'] = 'HA_RI_Max1'\n",
    "Zones.append(HA_RI_Max1) # add index to list\n",
    "HA_RI_Max2 = HA_riffle.loc[(HA_riffle['stream_order'] == MaximumStreamOrder-2)]\n",
    "HA_RI_Max2['ZoneId'] = 'HA_RI_Max2'\n",
    "Zones.append(HA_RI_Max2) # add index to list\n",
    "\n",
    "# ----------- steep -----------------------\n",
    "HA_steep = HighAltitude.loc[(HighAltitude['slope'] > 0.04) & (HighAltitude['slope'] <= 0.1)]\n",
    "\n",
    "HA_ST_Max = HA_steep.loc[(HA_steep['stream_order'] == MaximumStreamOrder)]\n",
    "HA_ST_Max['ZoneId'] = 'HA_ST_Max'\n",
    "Zones.append(HA_ST_Max) # add index to list\n",
    "HA_ST_Max1 = HA_steep.loc[(HA_steep['stream_order'] == MaximumStreamOrder-1)]\n",
    "HA_ST_Max1['ZoneId'] = 'HA_ST_Max1'\n",
    "Zones.append(HA_ST_Max1) # add index to list\n",
    "HA_ST_Max2 = HA_steep.loc[(HA_steep['stream_order'] == MaximumStreamOrder-2)]\n",
    "HA_ST_Max2['ZoneId'] = 'HA_ST_Max2'\n",
    "Zones.append(HA_ST_Max2) # add index to list\n",
    "\n",
    "# --------------- very steep ----------------\n",
    "HA_verysteep = HighAltitude.loc[(HighAltitude['slope'] > 0.1)]\n",
    "\n",
    "HA_VS_Max = HA_verysteep.loc[(HA_verysteep['stream_order'] == MaximumStreamOrder)]\n",
    "HA_VS_Max['ZoneId'] = 'HA_VS_Max'\n",
    "Zones.append(HA_VS_Max) # add index to list\n",
    "HA_VS_Max1 = HA_verysteep.loc[(HA_verysteep['stream_order'] == MaximumStreamOrder-1)]\n",
    "HA_VS_Max1['ZoneId'] = 'HA_VS_Max1'\n",
    "Zones.append(HA_VS_Max1) # add index to list\n",
    "HA_VS_Max2 = HA_verysteep.loc[(HA_verysteep['stream_order'] == MaximumStreamOrder-2)]\n",
    "HA_VS_Max2['ZoneId'] = 'HA_VS_Max2'\n",
    "Zones.append(HA_VS_Max2) # add index to list\n",
    "\n",
    "# convert to dataframe\n",
    "ZonesData = pd.concat(Zones, axis = 0, ignore_index = True)    # convert to dataframe\n",
    "    \n",
    "nameCSV = 'Zones' + '.csv'\n",
    "AllDataPath = os.path.join(output, nameCSV)  \n",
    "ZonesData.to_csv(AllDataPath, sep=';', mode=\"w\", header=True, index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% calculate simple discharge statistics (5P, median, 95P)\n",
    "\n",
    "qr_median = np.median(qr, axis = 0)\n",
    "qr_5p = np.percentile(qr,5, axis =0)\n",
    "qr_95p = np.percentile(qr,95, axis =0)\n",
    "\n",
    "# add discharge stats to coordinates\n",
    "data_step = pd.DataFrame(data=qr_median)\n",
    "# turn dataframe upside down to match the static data..?\n",
    "data_step = pd.DataFrame(data=data_step).iloc[::-1]\n",
    "medianQ = []\n",
    "for i in range(len(long)):\n",
    "    df3 = pd.DataFrame(data_step[i])\n",
    "    frames = [df3]\n",
    "    data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    medianQ.append(data)\n",
    "medianQ_complete = pd.concat(medianQ, axis = 0, ignore_index = True)   \n",
    "\n",
    "\n",
    "data_step = pd.DataFrame(data=qr_5p)\n",
    "# turn dataframe upside down to match the static data..?\n",
    "data_step = pd.DataFrame(data=data_step).iloc[::-1]\n",
    "qr_5p = []\n",
    "for i in range(len(long)):\n",
    "    df3 = pd.DataFrame(data_step[i])\n",
    "    frames = [df3]\n",
    "    data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    qr_5p.append(data)\n",
    "qr_5p_complete = pd.concat(qr_5p, axis = 0, ignore_index = True)   \n",
    "\n",
    "data_step = pd.DataFrame(data=qr_95p)\n",
    "# turn dataframe upside down to match the static data..?\n",
    "data_step = pd.DataFrame(data=data_step).iloc[::-1]\n",
    "qr_95p = []\n",
    "for i in range(len(long)):\n",
    "    df3 = pd.DataFrame(data_step[i])\n",
    "    frames = [df3]\n",
    "    data = pd.concat(frames, axis = 1, ignore_index = True)    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    qr_95p.append(data)\n",
    "qr_95p_complete = pd.concat(qr_95p, axis = 0, ignore_index = True)   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydromt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac595f5bbfb47e641a8dd8285cfda8dd9ec1b2f441d90148123236945b84d5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
